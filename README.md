# üî¨ Proyecto Higgs Boson - Clasificaci√≥n H‚ÜíWW* vs DibosonWW

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![XGBoost](https://img.shields.io/badge/XGBoost-3.1+-green.svg)](https://xgboost.readthedocs.io/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

Proyecto de Machine Learning para clasificaci√≥n de eventos de f√≠sica de part√≠culas del detector ATLAS en el LHC (CERN). El objetivo es distinguir eventos de desintegraci√≥n del bos√≥n de Higgs (H‚ÜíWW*) del fondo de producci√≥n directa de dibosones (WW).

---

## üìä **Dataset**

- **Eventos totales**: 26,277
  - **Se√±al (Higgs)**: 11,340 eventos (43.2%)
  - **Fondo (WW)**: 14,937 eventos (56.8%)
- **Features originales**: 35 variables cinem√°ticas y topol√≥gicas
- **Features ingenieradas**: +15 variables f√≠sicas derivadas
- **Fuente**: Datos simulados de ATLAS (Monte Carlo)

### Variables clave:
| Variable | Descripci√≥n | Importancia |
|----------|-------------|-------------|
| `mLL` | Masa invariante dilept√≥nica | Alta |
| `pTll` | Momento transverso del sistema ll | Alta |
| `met_et` | Energ√≠a transversa faltante (MET) | Alta |
| `dphi_ll_met` | √Ångulo azimutal entre ll y MET | Alta |
| `delta_R_ll` | Separaci√≥n angular entre leptones | Media |
| `MT_ll_met` | Masa transversa ll-MET | Alta |

---

## üöÄ **Instalaci√≥n**

### 1. Clonar el repositorio
```bash
git clone <repository-url>
cd Higgs
```

### 2. Crear entorno virtual
```bash
# Crear entorno
python -m venv venv

# Activar (Windows PowerShell)
.\venv\Scripts\Activate.ps1

# Activar (Linux/Mac)
source venv/bin/activate
```

### 3. Instalar dependencias
```bash
pip install -r requirements.txt
```

**Paquetes principales:**
- `pandas>=2.0` - Manipulaci√≥n de datos
- `numpy>=1.24` - Computaci√≥n num√©rica
- `scikit-learn>=1.3` - Machine Learning
- `xgboost>=3.1` - Gradient Boosting
- `lightgbm>=4.0` - Gradient Boosting alternativo
- `catboost>=1.2` - Gradient Boosting alternativo
- `optuna>=3.0` - Optimizaci√≥n bayesiana
- `shap>=0.40` - Interpretabilidad
- `matplotlib>=3.7` - Visualizaci√≥n
- `seaborn>=0.12` - Visualizaci√≥n estad√≠stica
- `jupyter>=1.0` - Notebooks

---

## üìÅ **Estructura del Proyecto**

```
Higgs/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                          # Datos originales
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datos_filtrados_Higgs.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datos_filtrados_DibosonWW.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Higgs8TeVPipeline.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ interim/                      # Datos procesados intermedios
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ merged_raw.pkl           # Dataset combinado
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ folded/                  # Folds para CV
‚îÇ   ‚îî‚îÄ‚îÄ processed/                    # Datos finales
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                        # Notebooks Jupyter
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_understanding.ipynb  # EDA y an√°lisis exploratorio
‚îÇ   ‚îú‚îÄ‚îÄ 02_pipeline.ipynb            # Pipeline completo de entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ 03_resultados.ipynb          # An√°lisis de resultados y m√©tricas
‚îÇ   ‚îî‚îÄ‚îÄ 04_mejora_modelo.ipynb       # Optimizaci√≥n e ingenier√≠a de features
‚îÇ
‚îú‚îÄ‚îÄ src/                              # C√≥digo fuente
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ load.py                  # Carga de datos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ merge_data.py            # Combinaci√≥n de datasets
‚îÇ   ‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ feature_engineering.py   # Ingenier√≠a de features
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ boosting.py              # Modelos de boosting
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trainer.py               # Entrenamiento
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metrics.py               # M√©tricas personalizadas
‚îÇ   ‚îú‚îÄ‚îÄ fold.split.py                # Estratificaci√≥n de folds
‚îÇ   ‚îî‚îÄ‚îÄ selectors.py                 # Selecci√≥n de features
‚îÇ
‚îú‚îÄ‚îÄ models/                           # Modelos entrenados
‚îÇ   ‚îú‚îÄ‚îÄ best_model.pkl               # Mejor modelo baseline
‚îÇ   ‚îú‚îÄ‚îÄ best_model_optimized.pkl     # Modelo optimizado
‚îÇ   ‚îú‚îÄ‚îÄ final_features.json          # Features seleccionadas
‚îÇ   ‚îú‚îÄ‚îÄ enhanced_features.json       # Features extendidas
‚îÇ   ‚îú‚îÄ‚îÄ best_hyperparams.json        # Hiperpar√°metros √≥ptimos
‚îÇ   ‚îî‚îÄ‚îÄ folds/
‚îÇ       ‚îî‚îÄ‚îÄ fold_results.csv         # Resultados por fold
‚îÇ
‚îú‚îÄ‚îÄ reports/                          # Reportes y figuras
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                  # Dependencias Python
‚îî‚îÄ‚îÄ README.md                         # Este archivo
```

---

## üéØ **Workflow del Proyecto**

### **Fase 1: Exploraci√≥n de Datos** (`01_data_understanding.ipynb`)
- Carga y verificaci√≥n del dataset
- An√°lisis de distribuciones (mLL, pTll, dphi_ll, etc.)
- Matriz de correlaci√≥n optimizada
- An√°lisis de balance de clases
- Identificaci√≥n de variables discriminantes
- Feature importance con Random Forest

**Salidas:**
- Visualizaciones de distribuciones
- Heatmap de correlaciones
- Conclusiones sobre estrategia de modelado

---

### **Fase 2: Pipeline de Entrenamiento** (`02_pipeline.ipynb`)
1. **Carga de datos**: `merged_raw.pkl`
2. **Feature engineering**: Variables f√≠sicas derivadas
3. **Generaci√≥n de folds**: StratifiedKFold (5-fold)
4. **Entrenamiento con CV**: XGBoost, LightGBM, CatBoost
5. **Selecci√≥n de features**: Importancia en ‚â•3 folds
6. **Modelo final**: Re-entrenamiento con features seleccionadas
7. **Validaci√≥n**: Evaluaci√≥n en conjunto completo

**M√©tricas calculadas:**
- AUC-ROC
- Accuracy
- F1-Score
- AMS (Approximate Median Significance)

**Salidas:**
- `models/best_model.pkl`
- `models/final_features.json`
- `models/folds/fold_results.csv`

---

### **Fase 3: An√°lisis de Resultados** (`03_resultados.ipynb`)
- M√©tricas promedio de CV
- Gr√°ficas de m√©tricas por fold
- Curva ROC del modelo final
- Matriz de confusi√≥n
- Importancia de variables (feature importance)
- An√°lisis SHAP (interpretabilidad)
- Conclusiones automatizadas

**Visualizaciones:**
- AUC, AMS, Accuracy, F1 por fold
- Curva ROC con AUC
- Matriz de confusi√≥n
- Top 20 features m√°s importantes
- SHAP summary plot
- SHAP bar plot

---

### **Fase 4: Optimizaci√≥n de Modelo** (`04_mejora_modelo.ipynb`)

#### **Estrategias implementadas:**

1. **Optimizaci√≥n Bayesiana (Optuna)**
   - 50 trials en espacio de hiperpar√°metros
   - Validaci√≥n cruzada 5-fold
   - B√∫squeda en 9 par√°metros clave

2. **Feature Engineering Avanzado**
   - 15 nuevas variables f√≠sicas
   - Variables derivadas: HT, centrality, ll_boost, etc.
   - Evaluaci√≥n individual de importancia

3. **An√°lisis Comparativo**
   - 5 estrategias probadas
   - Selecci√≥n autom√°tica del mejor modelo
   - Comparaci√≥n multi-m√©trica

#### **Modelos comparados:**
| Estrategia | Descripci√≥n | Features |
|------------|-------------|----------|
| Baseline | Modelo original | 15 |
| Optuna + 8 Features | Hiperpar√°metros optimizados + top 8 | 23 |
| Solo Hyperparams | Optimizaci√≥n sin nuevas features | 15 |
| Original + Top 3 | Conservador | 18 |
| Original + Top 5 | H√≠brido | 20 |

**Salidas:**
- `models/best_model_optimized.pkl`
- `models/enhanced_features.json`
- `models/best_hyperparams.json`
- Tabla comparativa de estrategias

---

## üìà **Resultados**

### **Baseline (Modelo Original)**
```
AUC:      0.8651
Accuracy: 0.7777
F1-Score: 0.7520
AMS:      117.60
Features: 15
```

### **Mejor Estrategia** (identificada autom√°ticamente)
- Ver output de `04_mejora_modelo.ipynb` celda final
- La mejor estrategia se selecciona din√°micamente por AUC

---

## üîß **Uso**

### **Entrenamiento desde cero**

```python
# 1. Preparar datos
from src.data.merge_data import merge_and_save
merge_and_save()

# 2. Ejecutar pipeline completo
# Ejecutar notebook: 02_pipeline.ipynb

# 3. Analizar resultados
# Ejecutar notebook: 03_resultados.ipynb

# 4. Optimizar modelo (opcional)
# Ejecutar notebook: 04_mejora_modelo.ipynb
```

### **Predicci√≥n con modelo entrenado**

```python
import joblib
import pandas as pd
from src.features.feature_engineering import add_feature_engineering

# Cargar modelo
model = joblib.load("models/best_model.pkl")

# Cargar features
with open("models/final_features.json", "r") as f:
    features = json.load(f)

# Preparar datos nuevos
df_new = pd.read_csv("new_data.csv")
df_new = add_feature_engineering(df_new)

# Predecir
X_new = df_new[features]
y_pred = model.predict_proba(X_new)[:, 1]  # Probabilidad de Higgs

# Clasificar
threshold = 0.5
predictions = (y_pred >= threshold).astype(int)
```

### **Optimizaci√≥n de hiperpar√°metros**

```python
import optuna
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 200, 1000, step=50),
        'max_depth': trial.suggest_int('max_depth', 4, 12),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        # ... m√°s par√°metros
    }
    model = XGBClassifier(**params)
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc')
    return scores.mean()

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)
print(f"Mejor AUC: {study.best_value:.4f}")
```

---

## üìä **M√©tricas de Evaluaci√≥n**

### **AUC-ROC** (Area Under the Curve)
- Rango: [0.5, 1.0]
- **Interpretaci√≥n**: Capacidad de discriminar entre Higgs y WW
- **Objetivo**: ‚â• 0.85 (excelente)

### **Accuracy** (Precisi√≥n Global)
- Rango: [0.0, 1.0]
- **Interpretaci√≥n**: % de eventos correctamente clasificados
- **Objetivo**: ‚â• 0.75

### **F1-Score** (Media arm√≥nica Precisi√≥n-Recall)
- Rango: [0.0, 1.0]
- **Interpretaci√≥n**: Balance entre precisi√≥n y sensibilidad
- **Objetivo**: ‚â• 0.70

### **AMS** (Approximate Median Significance)
- Rango: [0, ‚àû)
- **F√≥rmula**: AMS = ‚àö(2¬∑((s+b)¬∑ln(1+s/b) - s))
- **Interpretaci√≥n**: Significancia estad√≠stica en f√≠sica de part√≠culas
- **Objetivo**: Maximizar (t√≠picamente > 50)

---

## üß† **Feature Engineering**

### **Features b√°sicas** (implementadas en `feature_engineering.py`)

```python
def add_feature_engineering(df):
    # Masa transversa ll-MET
    df['MT_ll_met'] = sqrt(2¬∑pTll¬∑MET¬∑(1-cos(ŒîœÜ)))
    
    # Separaci√≥n angular entre leptones
    df['delta_R_ll'] = sqrt(ŒîŒ∑¬≤ + ŒîœÜ¬≤)
    
    # Ratio de momento transverso
    df['pt_ratio'] = lep_pt_0 / lep_pt_1
    
    # ... +8 variables m√°s
    return df
```

### **Features avanzadas** (para optimizaci√≥n)

```python
def add_advanced_features(df):
    # Energ√≠a transversa total
    df['HT'] = lep_pt_0 + lep_pt_1 + MET
    
    # Centrality (posici√≥n en detector)
    df['centrality'] = (lep_eta_0¬≤ + lep_eta_1¬≤) / 2
    
    # Boost del sistema dilept√≥nico
    df['ll_boost'] = sqrt(pTll¬≤ + mLL¬≤)
    
    # ... +12 variables m√°s
    return df
```

---

## üîç **Interpretabilidad**

### **SHAP (SHapley Additive exPlanations)**

```python
import shap

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)

# Summary plot
shap.summary_plot(shap_values, X, plot_type="dot")

# Bar plot
shap.summary_plot(shap_values, X, plot_type="bar")
```

**Interpretaci√≥n:**
- **Rojo**: Valores altos de la feature
- **Azul**: Valores bajos de la feature
- **Eje X**: Impacto en la predicci√≥n (positivo = m√°s probable Higgs)

---

## üõ†Ô∏è **Configuraci√≥n Avanzada**

### **Hiperpar√°metros XGBoost recomendados**

```python
params = {
    'n_estimators': 500,          # N√∫mero de √°rboles
    'max_depth': 6,               # Profundidad m√°xima
    'learning_rate': 0.1,         # Tasa de aprendizaje
    'subsample': 0.8,             # Fracci√≥n de datos por √°rbol
    'colsample_bytree': 0.8,      # Fracci√≥n de features por √°rbol
    'min_child_weight': 3,        # Peso m√≠nimo por hoja
    'gamma': 0.1,                 # Regularizaci√≥n m√≠nima para split
    'reg_alpha': 0.01,            # L1 regularization
    'reg_lambda': 1.0,            # L2 regularization
    'random_state': 42,
    'eval_metric': 'auc'
}
```

### **Validaci√≥n Cruzada Estratificada**

```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
    
    model.fit(X_train, y_train)
    # ... evaluar
```

---

## üêõ **Troubleshooting**

### **Error: ModuleNotFoundError: No module named 'src'**
```python
# Agregar al inicio del notebook
import sys
from pathlib import Path
sys.path.append(str(Path.cwd().parent))
```

### **Error: KeyError en features**
```bash
# Verificar que las features existen
python -c "import pandas as pd; df = pd.read_pickle('data/interim/merged_raw.pkl'); print(df.columns.tolist())"
```

### **Error: ams_score con arrays**
```python
# La funci√≥n actualizada acepta arrays completos
from src.models.boosting import ams_score
score = ams_score(y_true, y_pred_proba)  # y_pred_proba son probabilidades
```

### **Error: Optuna no encuentra mejor trial**
```python
# Aumentar n√∫mero de trials
study.optimize(objective, n_trials=100)  # En lugar de 50
```

---

## üìö **Referencias**

### **F√≠sica**
- [ATLAS Collaboration](https://atlas.cern/)
- [Higgs Discovery Paper (2012)](https://www.sciencedirect.com/science/article/pii/S037026931200857X)
- [H‚ÜíWW* Analysis](https://arxiv.org/abs/1412.2641)

### **Machine Learning**
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [Optuna Paper](https://arxiv.org/abs/1907.10902)
- [SHAP Documentation](https://shap.readthedocs.io/)

### **Kaggle Competition**
- [Higgs Boson Challenge (2014)](https://www.kaggle.com/c/higgs-boson)
- [AMS Metric Explanation](https://www.kaggle.com/c/higgs-boson/overview/evaluation)

---

## ü§ù **Contribuciones**

¬°Las contribuciones son bienvenidas! Para contribuir:

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/nueva-feature`)
3. Commit tus cambios (`git commit -am 'Agrega nueva feature'`)
4. Push a la rama (`git push origin feature/nueva-feature`)
5. Abre un Pull Request

---

## üìù **TODO / Mejoras Futuras**

- [ ] Implementar ensemble stacking (XGB + LGBM + CatBoost)
- [ ] Agregar calibraci√≥n de probabilidades (Platt scaling)
- [ ] Optimizaci√≥n de threshold para maximizar AMS
- [ ] Implementar data augmentation (SMOTE)
- [ ] Agregar modelo de Deep Learning (MLP)
- [ ] API REST para predicciones en tiempo real
- [ ] Dashboard interactivo con Streamlit/Dash
- [ ] Monitoreo de data drift en producci√≥n
- [ ] Tests unitarios para m√≥dulos cr√≠ticos
- [ ] Documentaci√≥n de API con Sphinx

---

## üë®‚Äçüíª **Autor**

**Tu Nombre**
- Email: tu.email@example.com
- GitHub: [@tu-usuario](https://github.com/tu-usuario)
- LinkedIn: [Tu Perfil](https://linkedin.com/in/tu-perfil)

---

## üìÑ **Licencia**

Este proyecto est√° bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para m√°s detalles.

---

## üôè **Agradecimientos**

- **CERN/ATLAS** por los datos simulados
- **Kaggle** por la competencia Higgs Boson Challenge
- **XGBoost Team** por la excelente herramienta
- **Optuna** por la optimizaci√≥n bayesiana eficiente
- **Comunidad de ML en F√≠sica de Part√≠culas**

---

## üìû **Soporte**

Si encuentras alg√∫n problema o tienes preguntas:

1. Revisa la secci√≥n [Troubleshooting](#troubleshooting)
2. Busca en [Issues](https://github.com/tu-usuario/Higgs/issues)
3. Abre un nuevo Issue con detalles del problema
4. Contacta al autor por email

---

**‚≠ê Si este proyecto te fue √∫til, considera darle una estrella en GitHub!**

---

*√öltima actualizaci√≥n: 30 de noviembre de 2025*
